{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDd2BDLuDzKa"
      },
      "source": [
        "# GPT-2 Chatbot\n",
        "\n",
        "*Using huggingface's gpt2-xl-pytorch model and implementation code from graycode.*\n",
        "\n",
        "\n",
        "- Run all codeblocks in order.\n",
        "\n",
        "- The last one will prompt you to enter a name for yourself as well as a name for the bot and then the conversation will begin.\n",
        "\n",
        "- This should be more polished in the future but for now, if you do not like a conversation or you are getting text artifacts (Such as more names coming into the conversation or if the bot talks twice in one line) say \"bye\" and the bot will reset and ask you for it's new name.\n",
        "\n",
        "- It works better with more common or believable names, so keep that in mind if gpt-2 is having trouble understanding that it is meant to be having a conversation.\n",
        "\n",
        "- If the model runs into a problem, simply rerun the setup and the chatbot will be ready to run again. No need to rerun the first block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIjdWcl2wnLO"
      },
      "source": [
        "## Download Model and Necessary Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEz7zVLtreRM"
      },
      "source": [
        "from google.colab import output\n",
        " \n",
        "#Download Dependencies\n",
        "!pip install regex==2017.4.5\n",
        "!git clone https://github.com/graykode/gpt-2-Pytorch\n",
        "%cd gpt-2-Pytorch\n",
        " \n",
        "# download huggingface's pytorch model \n",
        "!curl --output gpt2-xl-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01NxNNYtweXB"
      },
      "source": [
        "## Run Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8udvDrNUbNP"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from google.colab import output\n",
        " \n",
        "if os.getcwd() != \"../gpt-2-Pytorch\":\n",
        "    %cd gpt-2-Pytorch\n",
        " \n",
        "def clear_screen():\n",
        "    output.clear()\n",
        "\n",
        "class GPT2Config(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size_or_config_json_file=50257,\n",
        "            n_positions=1024,\n",
        "            n_ctx=1024,\n",
        "            n_embd=1600,\n",
        "            n_layer=48,\n",
        "            n_head=25,\n",
        "            layer_norm_epsilon=1e-5,\n",
        "            initializer_range=0.02,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size_or_config_json_file\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n",
        "        \n",
        "%run GPT2/encoder\n",
        "%run GPT2/model\n",
        "%run GPT2/utils\n",
        " \n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1]\n",
        "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
        " \n",
        "def sample_sequence(model, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True,):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
        "    prev = context\n",
        "    output = context\n",
        "    past = None\n",
        "    enc = get_encoder()\n",
        "    end_type = \"\"\n",
        "    resp_name = None\n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        current_token = enc.decode(output[:, len(context):].tolist()[-1]).split(\" \")[-1]\n",
        "        while count < 1 or (not \":\" in current_token and not \"\\n\" in current_token):\n",
        "            logits, past = model(prev, past=past)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            log_probs = F.softmax(logits, dim=-1)\n",
        "            if sample:\n",
        "                prev = torch.multinomial(log_probs, num_samples=1)\n",
        "            else:\n",
        "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
        "            output = torch.cat((output, prev), dim=1)\n",
        "            count += 1\n",
        "            if count > 100:\n",
        "                end_type = \"max\"\n",
        "                break\n",
        "            current_token = enc.decode(output[:, len(context):].tolist()[-1]).split(\" \")[-1]\n",
        "        if \":\" in current_token:\n",
        "            end_type = \"colon\"\n",
        "        elif \"\\n\" in current_token:\n",
        "            end_type = \"newline\"\n",
        "\n",
        "    return output, end_type\n",
        " \n",
        "def load_model(state_dict):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--text\", type=str, required=False)\n",
        "    parser.add_argument(\"--quiet\", type=bool, default=False)\n",
        "    parser.add_argument(\"--nsamples\", type=int, default=1)\n",
        "    parser.add_argument('--unconditional', action='store_true', help='If true, unconditional generation.')\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=-1)\n",
        "    parser.add_argument(\"--length\", type=int, default=1)\n",
        "    parser.add_argument(\"--temperature\", type=float, default=0.7)\n",
        "    parser.add_argument(\"--top_k\", type=int, default=40)\n",
        "    args = parser.parse_args([])\n",
        " \n",
        "    if args.quiet is False:\n",
        "        print(args)\n",
        " \n",
        "    if args.batch_size == -1:\n",
        "        args.batch_size = 1\n",
        "    assert args.nsamples % args.batch_size == 0\n",
        "    print(\"Enter a seed or a negative number for random:\")\n",
        "    seed = (int)(input())\n",
        "    if seed < 0:\n",
        "        seed = random.randint(0, 2147483647)\n",
        "        print(\"Random seed: \", seed)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        " \n",
        "    enc = get_encoder()\n",
        "    config = GPT2Config()\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    model = load_weight(model, state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return [model, config, enc, args, device], seed\n",
        " \n",
        "def generate_reply(model, config, enc, args, device):\n",
        "    if args.length == -1:\n",
        "        args.length = config.n_ctx // 2\n",
        "    elif args.length > config.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
        "\n",
        "    context_tokens = enc.encode(args.text)\n",
        "    output = \"\"\n",
        "    resp_name = None\n",
        "    generated = 0\n",
        "    for _ in range(args.nsamples // args.batch_size):\n",
        "        out, end_type = sample_sequence(\n",
        "            model=model,\n",
        "            context=context_tokens  if not  args.unconditional else None,\n",
        "            start_token=enc.encoder['<|endoftext|>'] if args.unconditional else None,\n",
        "            batch_size=args.batch_size,\n",
        "            temperature=args.temperature, top_k=args.top_k, device=device\n",
        "        )\n",
        "        out = out[:, len(context_tokens):].tolist()\n",
        "        for i in range(args.batch_size):\n",
        "            generated += 1\n",
        "            text = enc.decode(out[i])\n",
        "            #if args.quiet is False:\n",
        "                #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "            #print(text)\n",
        "            output += text\n",
        "    if end_type == \"newline\":\n",
        "    \toutput = output.rstrip('\\n')\n",
        "    elif end_type == \"colon\":\n",
        "        split = output.rstrip().rsplit(' ', 1)\n",
        "        resp_name = split[1].rstrip(':')\n",
        "        output = split[0]\n",
        "    elif end_type == \"max\":\n",
        "    \toutput = output.rsplit('.|?|!', 1)[0]\n",
        "    return output, resp_name\n",
        "\n",
        "\n",
        "if os.path.exists('gpt2-xl-pytorch_model.bin'):\n",
        "    state_dict = torch.load('gpt2-xl-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
        "    model, seed = load_model(state_dict)\n",
        "else:\n",
        "    print('Please download gpt2-xl-pytorch_model.bin')\n",
        "    sys.exit()\n",
        "\n",
        "def run():\n",
        "    conv_so_far = \"\"\n",
        "    name = \"\"\n",
        "    my_name = \"\"\n",
        "    clear_screen()\n",
        "    print(\"Enter your name (No Spaces.):\")\n",
        "    my_name = input()\n",
        "    clear_screen()\n",
        "    while \" \" in my_name:\n",
        "        print(\"Please enter a valid name (No Spaces.):\")\n",
        "        my_name = input()\n",
        "        clear_screen()\n",
        "    print(\"Enter a name (No Spaces.):\")\n",
        "    name = input()\n",
        "    clear_screen()\n",
        "    while \" \" in name:\n",
        "        print(\"Please enter a valid name (No Spaces.):\")\n",
        "        name = input()\n",
        "        clear_screen()\n",
        "    in_conv = [name]\n",
        "    clear_screen()\n",
        "    print(\"Seed: \", seed)\n",
        "    print(f\"Start a conversation with {name} or say \\\"Bye\\\" at any point to reset personality.\\n\")\n",
        "    while True:\n",
        "        msg = input(f\"{my_name}: \")\n",
        "        if msg.lower() == \"bye\":\n",
        "            conv_so_far = \"\"\n",
        "            in_conv = [name]\n",
        "            clear_screen()\n",
        "            print(\"Enter a name (No Spaces.):\")\n",
        "            name = input()\n",
        "            clear_screen()\n",
        "            while \" \" in name:\n",
        "                print(\"Please enter a valid name (No Spaces.):\")\n",
        "                name = input()\n",
        "                clear_screen()\n",
        "            in_conv = [name]\n",
        "            print(\"Seed: \", seed)\n",
        "            print(f\"Start a conversation with {name} or say \\\"Bye\\\" at any point to reset personality.\\n\")\n",
        "            continue\n",
        "        who_talks = random.choice(in_conv)\n",
        "        conv_so_far += f\" {my_name}: \" + msg + \" \" + who_talks + \":\"\n",
        "        model[3].text = conv_so_far\n",
        "        answer, resp_name = generate_reply(model[0], model[1], model[2], model[3], model[4]) #.rstrip().rstrip(f\"{my_name}:\")\n",
        "        conv_so_far += answer\n",
        "        print(who_talks + \": \" + answer.strip(' '))\n",
        "        if resp_name is not None and resp_name not in in_conv and resp_name != my_name:\n",
        "            print(f\"{resp_name} would like to join the conversation, will you accept? (y/n):\")\n",
        "            join = input()\n",
        "            if join[0] == 'y':\n",
        "                in_conv.append(resp_name)\n",
        "\n",
        "clear_screen()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M4oNGzPxPvI"
      },
      "source": [
        "## Run ChatBot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc_78p6FbxdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42e2e80-91ee-4100-b4e8-d2c1603ad9fa"
      },
      "source": [
        "run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a name (No Spaces.):\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}